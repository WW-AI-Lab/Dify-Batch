"""
æ‰¹é‡å¤„ç†å™¨ - æ ¸å¿ƒæ‰¹é‡æ‰§è¡Œå¼•æ“
"""
import asyncio
import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from sqlalchemy.orm import selectinload

from app.core.logging import get_logger
from app.core.database import get_db_session
from app.core.exceptions import DifyAPIException, FileProcessingException
from app.models.batch_task import BatchTask, TaskExecution, ExecutionLog, TaskStatus, ExecutionStatus, LogLevel
from app.services.dify.client import DifyClient
from app.services.dify.mock_client import MockDifyClient  # å¯¼å…¥æ¨¡æ‹Ÿå®¢æˆ·ç«¯
from app.services.file import ExcelService
from .progress_tracker import ProgressTracker

logger = get_logger(__name__)

# æµ‹è¯•æ¨¡å¼æ ‡å¿— - ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®ä¸­è·å–
import os
TEST_MODE = os.getenv('TEST_MODE', 'false').lower() == 'true'

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.excel_service = ExcelService()
        self._running_tasks: Dict[str, asyncio.Task] = {}
        self._task_semaphores: Dict[str, asyncio.Semaphore] = {}
    
    async def start_batch_task(
        self,
        batch_task_id: str,
        workflow_config: Dict[str, Any],
        progress_callback: Optional[Callable] = None,
        is_recovery: bool = False
    ) -> bool:
        """
        å¯åŠ¨æ‰¹é‡ä»»åŠ¡
        
        Args:
            batch_task_id: æ‰¹é‡ä»»åŠ¡ID
            workflow_config: å·¥ä½œæµé…ç½®ï¼ˆåŒ…å«base_urlå’Œapi_keyï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            is_recovery: æ˜¯å¦ä¸ºæ¢å¤æ¨¡å¼
            
        Returns:
            æ˜¯å¦å¯åŠ¨æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹å¯åŠ¨æ‰¹é‡ä»»åŠ¡: {batch_task_id}")
            
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²åœ¨è¿è¡Œ
            if batch_task_id in self._running_tasks:
                logger.warning(f"æ‰¹é‡ä»»åŠ¡å·²åœ¨è¿è¡Œ: {batch_task_id}")
                return False
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            if is_recovery:
                task = asyncio.create_task(
                    self._resume_batch_task(batch_task_id, workflow_config, progress_callback)
                )
            else:
                task = asyncio.create_task(
                    self._execute_batch_task(batch_task_id, workflow_config, progress_callback)
                )
            self._running_tasks[batch_task_id] = task
            
            logger.info(f"æ‰¹é‡ä»»åŠ¡å¯åŠ¨æˆåŠŸ: {batch_task_id}")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æ‰¹é‡ä»»åŠ¡å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
            return False
    
    async def stop_batch_task(self, batch_task_id: str) -> bool:
        """
        åœæ­¢æ‰¹é‡ä»»åŠ¡
        
        Args:
            batch_task_id: æ‰¹é‡ä»»åŠ¡ID
            
        Returns:
            æ˜¯å¦åœæ­¢æˆåŠŸ
        """
        try:
            if batch_task_id not in self._running_tasks:
                logger.warning(f"æ‰¹é‡ä»»åŠ¡æœªåœ¨è¿è¡Œ: {batch_task_id}")
                return False
            
            # å–æ¶ˆä»»åŠ¡
            task = self._running_tasks[batch_task_id]
            task.cancel()
            
            try:
                await task
            except asyncio.CancelledError:
                pass
            
            # æ¸…ç†èµ„æº
            del self._running_tasks[batch_task_id]
            if batch_task_id in self._task_semaphores:
                del self._task_semaphores[batch_task_id]
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            async with get_db_session() as db:
                await db.execute(
                    update(BatchTask)
                    .where(BatchTask.id == batch_task_id)
                    .values(
                        status=TaskStatus.CANCELLED,
                        completed_at=datetime.utcnow()
                    )
                )
                await db.commit()
            
            logger.info(f"æ‰¹é‡ä»»åŠ¡å·²åœæ­¢: {batch_task_id}")
            return True
            
        except Exception as e:
            logger.error(f"åœæ­¢æ‰¹é‡ä»»åŠ¡å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
            return False
    
    async def pause_batch_task(self, batch_task_id: str) -> bool:
        """
        æš‚åœæ‰¹é‡ä»»åŠ¡
        
        Args:
            batch_task_id: æ‰¹é‡ä»»åŠ¡ID
            
        Returns:
            æ˜¯å¦æš‚åœæˆåŠŸ
        """
        try:
            async with get_db_session() as db:
                result = await db.execute(
                    select(BatchTask).where(BatchTask.id == batch_task_id)
                )
                batch_task = result.scalar_one_or_none()
                
                if not batch_task or batch_task.status != TaskStatus.RUNNING:
                    return False
                
                # æ›´æ–°çŠ¶æ€ä¸ºæš‚åœ
                await db.execute(
                    update(BatchTask)
                    .where(BatchTask.id == batch_task_id)
                    .values(status=TaskStatus.PAUSED)
                )
                await db.commit()
            
            logger.info(f"æ‰¹é‡ä»»åŠ¡å·²æš‚åœ: {batch_task_id}")
            return True
            
        except Exception as e:
            logger.error(f"æš‚åœæ‰¹é‡ä»»åŠ¡å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
            return False
    
    async def resume_batch_task(self, batch_task_id: str) -> bool:
        """
        æ¢å¤æ‰¹é‡ä»»åŠ¡
        
        Args:
            batch_task_id: æ‰¹é‡ä»»åŠ¡ID
            
        Returns:
            æ˜¯å¦æ¢å¤æˆåŠŸ
        """
        try:
            async with get_db_session() as db:
                result = await db.execute(
                    select(BatchTask).where(BatchTask.id == batch_task_id)
                )
                batch_task = result.scalar_one_or_none()
                
                if not batch_task or batch_task.status != TaskStatus.PAUSED:
                    return False
                
                # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
                await db.execute(
                    update(BatchTask)
                    .where(BatchTask.id == batch_task_id)
                    .values(status=TaskStatus.RUNNING)
                )
                await db.commit()
            
            logger.info(f"æ‰¹é‡ä»»åŠ¡å·²æ¢å¤: {batch_task_id}")
            return True
            
        except Exception as e:
            logger.error(f"æ¢å¤æ‰¹é‡ä»»åŠ¡å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
            return False
    
    async def _execute_batch_task(
        self,
        batch_task_id: str,
        workflow_config: Dict[str, Any],
        progress_callback: Optional[Callable] = None
    ):
        """
        æ‰§è¡Œæ‰¹é‡ä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘
        """
        start_time = time.time()
        
        try:
            async with get_db_session() as db:
                # è·å–æ‰¹é‡ä»»åŠ¡ä¿¡æ¯
                result = await db.execute(
                    select(BatchTask).where(BatchTask.id == batch_task_id)
                )
                batch_task = result.scalar_one_or_none()
                
                if not batch_task:
                    logger.error(f"æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨: {batch_task_id}")
                    return
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
                batch_task.status = TaskStatus.RUNNING
                batch_task.started_at = datetime.utcnow()
                await db.commit()
                
                logger.info(f"å¼€å§‹æ‰§è¡Œæ‰¹é‡ä»»åŠ¡: {batch_task.name}")
                
                # è§£æExcelæ–‡ä»¶
                if not batch_task.file_path or not Path(batch_task.file_path).exists():
                    raise FileProcessingException("æ‰¹é‡ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨")
                
                data_rows, columns = self.excel_service.parse_excel_file(batch_task.file_path)
                
                if not data_rows:
                    raise FileProcessingException("Excelæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                
                # æ›´æ–°æ€»é¡¹ç›®æ•°
                batch_task.total_items = len(data_rows)
                await db.commit()
                
                # åˆ›å»ºä»»åŠ¡æ‰§è¡Œè®°å½•
                task_executions = []
                for idx, row_data in enumerate(data_rows):
                    execution = TaskExecution(
                        batch_task_id=batch_task_id,
                        row_index=idx,
                        inputs=row_data,
                        status=ExecutionStatus.PENDING
                    )
                    task_executions.append(execution)
                    db.add(execution)
                
                await db.commit()
                
                # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
                semaphore = asyncio.Semaphore(batch_task.max_concurrency)
                self._task_semaphores[batch_task_id] = semaphore
                
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
                tasks = []
                for execution in task_executions:
                    task = asyncio.create_task(
                        self._execute_single_task(
                            batch_task_id, 
                            execution.id, 
                            execution.inputs,
                            semaphore,
                            batch_task.retry_count,
                            batch_task.timeout_seconds,
                            workflow_config  # ä¼ é€’é…ç½®è€Œä¸æ˜¯å®¢æˆ·ç«¯å®ä¾‹
                        )
                    )
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                await asyncio.gather(*tasks, return_exceptions=True)
                
                # æ›´æ–°æœ€ç»ˆçŠ¶æ€
                await self._finalize_batch_task(batch_task_id, start_time)
                
                if progress_callback:
                    await progress_callback(batch_task_id, 100, "completed")
                
        except asyncio.CancelledError:
            logger.info(f"æ‰¹é‡ä»»åŠ¡è¢«å–æ¶ˆ: {batch_task_id}")
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
            await self._handle_batch_task_error(batch_task_id, str(e))
        finally:
            # æ¸…ç†èµ„æº
            if batch_task_id in self._running_tasks:
                del self._running_tasks[batch_task_id]
            if batch_task_id in self._task_semaphores:
                del self._task_semaphores[batch_task_id]
    
    async def _resume_batch_task(
        self,
        batch_task_id: str,
        workflow_config: Dict[str, Any],
        progress_callback: Optional[Callable] = None
    ):
        """
        æ¢å¤æ‰¹é‡ä»»åŠ¡çš„æ‰§è¡Œï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        start_time = time.time()
        
        try:
            async with get_db_session() as db:
                # è·å–æ‰¹é‡ä»»åŠ¡ä¿¡æ¯
                result = await db.execute(
                    select(BatchTask).where(BatchTask.id == batch_task_id)
                )
                batch_task = result.scalar_one_or_none()
                
                if not batch_task:
                    logger.error(f"æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨: {batch_task_id}")
                    return
                
                logger.info(f"ğŸ”„ å¼€å§‹æ¢å¤æ‰¹é‡ä»»åŠ¡: {batch_task.name}")
                
                # ç¡®ä¿ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
                batch_task.status = TaskStatus.RUNNING
                if not batch_task.started_at:
                    batch_task.started_at = datetime.utcnow()
                await db.commit()
                
                # è·å–æ‰€æœ‰å¾…å¤„ç†çš„æ‰§è¡Œè®°å½•
                result = await db.execute(
                    select(TaskExecution)
                    .where(
                        TaskExecution.batch_task_id == batch_task_id,
                        TaskExecution.status == ExecutionStatus.PENDING
                    )
                    .order_by(TaskExecution.row_index)
                )
                pending_executions = result.scalars().all()
                
                if not pending_executions:
                    logger.info(f"æ²¡æœ‰å¾…å¤„ç†çš„å­ä»»åŠ¡ï¼Œæ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€: {batch_task_id}")
                    await self._finalize_batch_task(batch_task_id, start_time)
                    return
                
                logger.info(f"ğŸ“‹ å‘ç° {len(pending_executions)} ä¸ªå¾…å¤„ç†çš„å­ä»»åŠ¡")
                
                # é‡æ–°è®¡ç®—ä»»åŠ¡ç»Ÿè®¡
                await self._recalculate_task_stats(batch_task_id)
                
                # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
                semaphore = asyncio.Semaphore(batch_task.max_concurrency)
                self._task_semaphores[batch_task_id] = semaphore
                
                # å¹¶å‘æ‰§è¡Œå¾…å¤„ç†çš„ä»»åŠ¡
                tasks = []
                for execution in pending_executions:
                    task = asyncio.create_task(
                        self._execute_single_task(
                            batch_task_id, 
                            execution.id, 
                            execution.inputs,
                            semaphore,
                            batch_task.retry_count,
                            batch_task.timeout_seconds,
                            workflow_config
                        )
                    )
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                await asyncio.gather(*tasks, return_exceptions=True)
                
                # æ›´æ–°æœ€ç»ˆçŠ¶æ€
                await self._finalize_batch_task(batch_task_id, start_time)
                
                if progress_callback:
                    await progress_callback(batch_task_id, 100, "completed")
                
                logger.info(f"âœ… æ‰¹é‡ä»»åŠ¡æ¢å¤å®Œæˆ: {batch_task.name}")
                
        except asyncio.CancelledError:
            logger.info(f"æ‰¹é‡ä»»åŠ¡æ¢å¤è¢«å–æ¶ˆ: {batch_task_id}")
            raise
        except Exception as e:
            logger.error(f"æ‰¹é‡ä»»åŠ¡æ¢å¤å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
            await self._handle_batch_task_error(batch_task_id, str(e))
        finally:
            # æ¸…ç†èµ„æº
            if batch_task_id in self._running_tasks:
                del self._running_tasks[batch_task_id]
            if batch_task_id in self._task_semaphores:
                del self._task_semaphores[batch_task_id]
    
    async def _recalculate_task_stats(self, batch_task_id: str):
        """
        é‡æ–°è®¡ç®—ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            batch_task_id: æ‰¹é‡ä»»åŠ¡ID
        """
        try:
            async with get_db_session() as db:
                # ç»Ÿè®¡å„çŠ¶æ€çš„å­ä»»åŠ¡æ•°é‡
                result = await db.execute(
                    select(TaskExecution)
                    .where(TaskExecution.batch_task_id == batch_task_id)
                )
                executions = result.scalars().all()
                
                if not executions:
                    return
                
                total_items = len(executions)
                completed_items = sum(1 for e in executions if e.status == ExecutionStatus.SUCCESS)
                failed_items = sum(1 for e in executions if e.status == ExecutionStatus.FAILED)
                pending_items = sum(1 for e in executions if e.status == ExecutionStatus.PENDING)
                
                # è®¡ç®—è¿›åº¦
                progress_percentage = ((completed_items + failed_items) / total_items) * 100 if total_items > 0 else 0
                
                # æ›´æ–°æ‰¹é‡ä»»åŠ¡ç»Ÿè®¡
                await db.execute(
                    update(BatchTask)
                    .where(BatchTask.id == batch_task_id)
                    .values(
                        total_items=total_items,
                        completed_items=completed_items,
                        failed_items=failed_items,
                        progress_percentage=progress_percentage
                    )
                )
                await db.commit()
                
                logger.info(f"ğŸ“Š é‡æ–°è®¡ç®—ä»»åŠ¡ç»Ÿè®¡: {batch_task_id} - æ€»è®¡:{total_items}, å®Œæˆ:{completed_items}, å¤±è´¥:{failed_items}, å¾…å¤„ç†:{pending_items}")
                
        except Exception as e:
            logger.error(f"é‡æ–°è®¡ç®—ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
    
    async def _execute_single_task(
        self,
        batch_task_id: str,
        execution_id: str,
        inputs: Dict[str, Any],
        semaphore: asyncio.Semaphore,
        max_retries: int,
        timeout_seconds: int,
        workflow_config: Dict[str, Any]
    ):
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        async with semaphore:
            retry_count = 0
            start_time = time.time()
            
            logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå•ä¸ªä»»åŠ¡")
            logger.info(f"   æ‰¹é‡ä»»åŠ¡ID: {batch_task_id}")
            logger.info(f"   æ‰§è¡ŒID: {execution_id}")
            logger.info(f"   è¾“å…¥å‚æ•°: {json.dumps(inputs, ensure_ascii=False, indent=2)}")
            logger.info(f"   æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")
            logger.info(f"   è¶…æ—¶æ—¶é—´: {timeout_seconds}ç§’")
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„Difyå®¢æˆ·ç«¯å®ä¾‹
            if TEST_MODE:
                logger.info("ğŸ­ ä½¿ç”¨æ¨¡æ‹ŸDifyå®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•")
                dify_client = MockDifyClient(
                    base_url=workflow_config["base_url"],
                    api_key=workflow_config["api_key"]
                )
            else:
                logger.info("ğŸ”— ä½¿ç”¨çœŸå®Difyå®¢æˆ·ç«¯")
                dify_client = DifyClient(
                    base_url=workflow_config["base_url"],
                    api_key=workflow_config["api_key"]
                )
            
            while retry_count <= max_retries:
                try:
                    logger.info(f"ğŸ”„ æ‰§è¡Œå°è¯• {retry_count + 1}/{max_retries + 1}")
                    
                    # æ›´æ–°æ‰§è¡ŒçŠ¶æ€
                    await self._update_execution_status(
                        execution_id, 
                        ExecutionStatus.RUNNING,
                        started_at=datetime.utcnow()
                    )
                    
                    # æ‰§è¡Œå·¥ä½œæµ
                    logger.info(f"ğŸ“¡ è°ƒç”¨Difyå·¥ä½œæµAPI...")
                    async with dify_client:
                        response = await asyncio.wait_for(
                            dify_client.execute_workflow(inputs),
                            timeout=timeout_seconds
                        )
                    
                    # å¤„ç†æˆåŠŸç»“æœ
                    execution_time = time.time() - start_time
                    logger.info(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
                    logger.info(f"   æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
                    logger.info(f"   å“åº”æ•°æ®: {json.dumps(response.data, ensure_ascii=False, indent=2)}")
                    
                    await self._update_execution_status(
                        execution_id,
                        ExecutionStatus.SUCCESS,
                        outputs=response.data,
                        execution_time_seconds=execution_time,
                        completed_at=datetime.utcnow()
                    )
                    
                    # æ›´æ–°æ‰¹é‡ä»»åŠ¡ç»Ÿè®¡
                    await self._update_batch_task_stats(batch_task_id, "completed")
                    logger.info(f"ğŸ“Š å·²æ›´æ–°æ‰¹é‡ä»»åŠ¡ç»Ÿè®¡ (completed)")
                    return
                    
                except Exception as e:
                    retry_count += 1
                    execution_time = time.time() - start_time
                    
                    logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥ [å°è¯• {retry_count}/{max_retries + 1}]")
                    logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                    logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
                    logger.error(f"   æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
                    
                    if retry_count <= max_retries:
                        wait_time = 2 ** retry_count
                        logger.warning(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)  # æŒ‡æ•°é€€é¿
                    else:
                        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
                        logger.error(f"ğŸ’€ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä»»åŠ¡æœ€ç»ˆå¤±è´¥")
                        await self._update_execution_status(
                            execution_id,
                            ExecutionStatus.FAILED,
                            error_message=str(e),
                            execution_time_seconds=execution_time,
                            retry_count=retry_count - 1,
                            completed_at=datetime.utcnow()
                        )
                        await self._update_batch_task_stats(batch_task_id, "failed")
                        logger.info(f"ğŸ“Š å·²æ›´æ–°æ‰¹é‡ä»»åŠ¡ç»Ÿè®¡ (failed)")
    
    async def _update_execution_status(self, execution_id: str, status: ExecutionStatus, **kwargs):
        """æ›´æ–°æ‰§è¡ŒçŠ¶æ€"""
        async with get_db_session() as db:
            update_data = {"status": status}
            update_data.update(kwargs)
            
            await db.execute(
                update(TaskExecution)
                .where(TaskExecution.id == execution_id)
                .values(**update_data)
            )
            await db.commit()
    
    async def _update_batch_task_stats(self, batch_task_id: str, result_type: str):
        """æ›´æ–°æ‰¹é‡ä»»åŠ¡ç»Ÿè®¡"""
        async with get_db_session() as db:
            if result_type == "completed":
                await db.execute(
                    update(BatchTask)
                    .where(BatchTask.id == batch_task_id)
                    .values(completed_items=BatchTask.completed_items + 1)
                )
            elif result_type == "failed":
                await db.execute(
                    update(BatchTask)
                    .where(BatchTask.id == batch_task_id)
                    .values(failed_items=BatchTask.failed_items + 1)
                )
            
            await db.commit()
    
    async def _finalize_batch_task(self, batch_task_id: str, start_time: float):
        """å®Œæˆæ‰¹é‡ä»»åŠ¡"""
        async with get_db_session() as db:
            result = await db.execute(
                select(BatchTask).where(BatchTask.id == batch_task_id)
            )
            batch_task = result.scalar_one_or_none()
            
            if not batch_task:
                return
            
            # è®¡ç®—æœ€ç»ˆçŠ¶æ€
            total_processed = batch_task.completed_items + batch_task.failed_items + batch_task.skipped_items
            
            if total_processed == batch_task.total_items:
                final_status = TaskStatus.COMPLETED
            else:
                final_status = TaskStatus.FAILED
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            batch_task.status = final_status
            batch_task.completed_at = datetime.utcnow()
            batch_task.progress_percentage = 100.0
            
            await db.commit()
            
            # ç”Ÿæˆç»“æœæ–‡ä»¶
            await self._generate_result_file(batch_task_id)
    
    async def _handle_batch_task_error(self, batch_task_id: str, error_message: str):
        """å¤„ç†æ‰¹é‡ä»»åŠ¡é”™è¯¯"""
        async with get_db_session() as db:
            await db.execute(
                update(BatchTask)
                .where(BatchTask.id == batch_task_id)
                .values(
                    status=TaskStatus.FAILED,
                    error_message=error_message,
                    completed_at=datetime.utcnow()
                )
            )
            await db.commit()
    
    async def _generate_result_file(self, batch_task_id: str):
        """ç”Ÿæˆç»“æœæ–‡ä»¶"""
        try:
            async with get_db_session() as db:
                # è·å–æ‰¹é‡ä»»åŠ¡ä¿¡æ¯
                result = await db.execute(
                    select(BatchTask).where(BatchTask.id == batch_task_id)
                )
                batch_task = result.scalar_one_or_none()
                
                if not batch_task or not batch_task.file_path:
                    return
                
                # è·å–æ‰€æœ‰æ‰§è¡Œç»“æœ
                result = await db.execute(
                    select(TaskExecution)
                    .where(TaskExecution.batch_task_id == batch_task_id)
                    .order_by(TaskExecution.row_index)
                )
                executions = result.scalars().all()
                
                # æ„å»ºç»“æœæ•°æ®
                results = []
                for execution in executions:
                    if execution.status == ExecutionStatus.SUCCESS:
                        # æå–å®é™…çš„è¾“å‡ºå†…å®¹
                        output_text = "æ‰§è¡ŒæˆåŠŸ"
                        if execution.outputs:
                            if isinstance(execution.outputs, dict):
                                # æ£€æŸ¥æ˜¯å¦æœ‰ outputs å­—æ®µï¼ˆDify API è¿”å›ç»“æ„ï¼‰
                                if 'outputs' in execution.outputs and isinstance(execution.outputs['outputs'], dict):
                                    outputs_dict = execution.outputs['outputs']
                                    # å¦‚æœå†…å±‚è¿˜æœ‰ outputs å­—æ®µï¼Œç»§ç»­é€’å½’å¤„ç†
                                    if 'outputs' in outputs_dict and isinstance(outputs_dict['outputs'], dict):
                                        outputs_dict = outputs_dict['outputs']
                                    
                                    # æå–æ‰€æœ‰é”®å€¼å¯¹çš„å†…å®¹
                                    output_parts = []
                                    for key, value in outputs_dict.items():
                                        if value is not None and str(value).strip():
                                            output_parts.append(str(value))
                                    
                                    if output_parts:
                                        output_text = '\n'.join(output_parts) if len(output_parts) > 1 else output_parts[0]
                                    else:
                                        output_text = "æ— è¾“å‡ºå†…å®¹"
                                
                                # å…¼å®¹æ—§çš„ç›´æ¥ output å­—æ®µæ ¼å¼
                                elif 'output' in execution.outputs:
                                    output_text = execution.outputs['output']
                                
                                # å¦‚æœç›´æ¥å°±æ˜¯ outputs çš„å†…å®¹ï¼ˆæ²¡æœ‰åµŒå¥—çš„ outputs å­—æ®µï¼‰
                                else:
                                    # æå–æ‰€æœ‰éç³»ç»Ÿå­—æ®µçš„å†…å®¹
                                    system_fields = {'id', 'workflow_id', 'status', 'error', 'elapsed_time', 
                                                   'total_tokens', 'total_steps', 'created_at', 'finished_at'}
                                    output_parts = []
                                    for key, value in execution.outputs.items():
                                        if key not in system_fields and value is not None and str(value).strip():
                                            output_parts.append(str(value))
                                    
                                    if output_parts:
                                        output_text = '\n'.join(output_parts) if len(output_parts) > 1 else output_parts[0]
                                    else:
                                        output_text = str(execution.outputs)
                            else:
                                output_text = str(execution.outputs)
                        
                        results.append({
                            "success": True,
                            "output": output_text
                        })
                    else:
                        results.append({
                            "success": False,
                            "error": execution.error_message or "æ‰§è¡Œå¤±è´¥"
                        })
                
                # ç”Ÿæˆç»“æœæ–‡ä»¶
                from app.core.config import settings
                result_dir = Path(settings.RESULT_DIR)
                result_dir.mkdir(exist_ok=True)
                
                result_filename = f"result_{batch_task_id}.xlsx"
                result_path = result_dir / result_filename
                
                self.excel_service.generate_result_file(
                    batch_task.file_path,
                    results,
                    str(result_path)
                )
                
                # æ›´æ–°ç»“æœæ–‡ä»¶è·¯å¾„
                batch_task.result_path = str(result_path)
                await db.commit()
                
                logger.info(f"ç»“æœæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {result_path}")
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç»“æœæ–‡ä»¶å¤±è´¥: {batch_task_id}, é”™è¯¯: {e}")
    
    def get_running_tasks(self) -> List[str]:
        """è·å–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨"""
        return list(self._running_tasks.keys())
    
    def is_task_running(self, batch_task_id: str) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        return batch_task_id in self._running_tasks 